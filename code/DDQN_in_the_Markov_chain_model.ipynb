{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Markov Chain Model\n",
    "\n",
    "The Markov chain model is the second model used in our thesis. It is significantly more complex than the simple probabilistic model, however, it is still quite rudimentary. The model was developed by Hult and Kiessling in their paper *[Algorithmic trading with Markov Chains](https://www.researchgate.net/publication/268032734_ALGORITHMIC_TRADING_WITH_MARKOV_CHAINS)*.\n",
    "\n",
    "In this model, the limit order book (LOB) is modelled explicitly. There are six event types:\n",
    "\n",
    "> 1. Buy limit order \n",
    "> 2. Sell limit order\n",
    "> 3. Cancel buy order\n",
    "> 4. Cancel sell order\n",
    "> 5. Buy market order\n",
    "> 6. Sell market order\n",
    "\n",
    "The arrival of an event results in a state transition in the Markov chain. The transition rates are described in our [report](deadlink). An example of how the arrival of different events affect the LOB is shown in the image below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LOBDynamics.png\" width=800/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Like in the simple probabilistic model we also have:\n",
    "\n",
    "> * The time _t_ can take integer values between _0_ and _T_.\n",
    ">\n",
    "> * The market maker can put the bid and ask depths at *max\\_quote\\_depth* different levels, from _1_ to *max\\_quote\\_depth* ticks away from the best ask and best bid price respectively.\n",
    ">\n",
    "> * The cash process _X<sub>t</sub>_ denotes the market makers cash at time _t_.\n",
    ">\n",
    "> * The inventory process _Q<sub>t</sub>_ denotes the market makers inventory at time _t_.\n",
    ">\n",
    "> * The market maker can see the current time _t_ , its inventory _Q<sub>t</sub>_ and the *full LOB* before taking an action.\n",
    ">\n",
    "> * At time _t_ the market maker is forced to liquidate its position.\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of AAPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The deep reinforcement learning\n",
    "\n",
    "After that short introduction, it's time for some deep reinforcement learning in the form of DDQN.\n",
    "\n",
    "We start by importing the needed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Q-learning file for the markov chain model\n",
    "from mc_model_mm_deep_rl_batch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to decide on the parameters we want to use for the environment and the hyperparameters we want to use for the DDQN.\n",
    "\n",
    "There are some additional parameters in the Markov chain model, see the code snippet below for an explanation of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"dt\": 1,                    # the length of the time steps\n",
    "                \"T\": 100,                   # the length of the episode\n",
    "                \"num_levels\": 10,           # how many depth levels that should be included in the LOB\n",
    "                \"default_order_size\": 5,    # the size of the orders the MM places\n",
    "                \"max_quote_depth\": 5,       # how deep the MM can put its quotes\n",
    "                \"reward_scale\": 0.1,        # a factor all rewards will be multiplied with\n",
    "                \"randomize_reset\": True     # should a random LOB state  be chosen at the start of every episode?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to decide which hyperparameter values we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "                \"n_train\": int(1e5),    # the number of steps the agents will be trained for\n",
    "                \"n_test\": int(1e2),     # the number of episodes the agents will be evaluated for\n",
    "                \"n_runs\": 4             # the number of agents that will be trained\n",
    "}\n",
    "\n",
    "DDQN_params = {\n",
    "                # network params\n",
    "                \"hidden_size\": 64,                                          # the hidden size of the network\n",
    "                \"buffer_size\": hyperparams[\"n_train\"] / 200,                # the size of the experience replay bank\n",
    "                \"replay_start_size\": hyperparams[\"n_train\"] / 200,          # after how many number of steps the experience replay is started\n",
    "                \"target_update_interval\": hyperparams[\"n_train\"] / 100,     # how often the target network is updated\n",
    "                \"update_interval\": 2,                                       # how often the online network is updated\n",
    "                \"minibatch_size\": 16,                                       # the size of the minibatches used\n",
    "\n",
    "                # epsilon greedy (linear decay)\n",
    "                \"exploration_initial_eps\": 1,                               # the starting value of the exploration rate\n",
    "                \"exploration_final_eps\": 0.05,                              # the final value of the exploration rate\n",
    "                \"exploration_fraction\": 0.5,                                # when the final value is reached\n",
    "\n",
    "                # learning rate\n",
    "                \"learning_rate_dqn\": 1e-4,                                  # the learning rate used (Adam)\n",
    "                \n",
    "                # other params\n",
    "                \"num_envs\": 10,                                             # how many parallelized environments\n",
    "                \"n_train\": hyperparams[\"n_train\"], \n",
    "                \"n_runs\": hyperparams[\"n_runs\"],\n",
    "                \"reward_scale\": model_params[\"reward_scale\"],\n",
    "\n",
    "                # logging params\n",
    "                \"log_interval\": hyperparams[\"n_train\"] / 100,               # the frequency of saving information\n",
    "                \"num_estimate\": 10000,                                      # how many states that should be used for estimating q_values\n",
    "                \"n_states\": 10                                              # the number of states heatmaps are averaged over\n",
    "                \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model it is the emulating that is the bottleneck, so it runs faster on a cpu than a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decide where to save our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the folder where the results will be saved\n",
    "folder_name = \"mc_deep_example\"\n",
    "\n",
    "outdir = \"results/mc_model_deep/\" + folder_name + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for the deep reinforcement learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 1 IN PROGRESS...\n",
      "\tStep 20000 (20%), 0:03:35.280000 remaining of this run\n",
      "\tStep 40000 (40%), 0:02:44.930000 remaining of this run\n",
      "\tStep 60000 (60%), 0:01:50.320000 remaining of this run\n",
      "\tStep 80000 (80%), 0:00:55.200000 remaining of this run\n",
      "\tStep 100000 (100%), 0:00:00 remaining of this run\n",
      "...FINISHED IN 0:04:39.860000\n",
      "0:13:59.580000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 2 IN PROGRESS...\n",
      "\tStep 20000 (20%), 0:03:29.910000 remaining of this run\n",
      "\tStep 40000 (40%), 0:02:44.750000 remaining of this run\n",
      "\tStep 60000 (60%), 0:01:50.650000 remaining of this run\n",
      "\tStep 80000 (80%), 0:00:55.420000 remaining of this run\n",
      "\tStep 100000 (100%), 0:00:00 remaining of this run\n",
      "...FINISHED IN 0:04:38.600000\n",
      "0:09:17.200000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 3 IN PROGRESS...\n",
      "\tStep 20000 (20%), 0:03:30.560000 remaining of this run\n",
      "\tStep 40000 (40%), 0:02:44.620000 remaining of this run\n",
      "\tStep 60000 (60%), 0:01:50.940000 remaining of this run\n",
      "\tStep 80000 (80%), 0:00:55.370000 remaining of this run\n",
      "\tStep 100000 (100%), 0:00:00 remaining of this run\n",
      "...FINISHED IN 0:04:38.880000\n",
      "0:04:38.880000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 4 IN PROGRESS...\n",
      "\tStep 20000 (20%), 0:03:30.390000 remaining of this run\n",
      "\tStep 40000 (40%), 0:02:46.570000 remaining of this run\n",
      "\tStep 60000 (60%), 0:01:50.650000 remaining of this run\n",
      "\tStep 80000 (80%), 0:00:55.170000 remaining of this run\n",
      "\tStep 100000 (100%), 0:00:00 remaining of this run\n",
      "...FINISHED IN 0:04:38.860000\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "train_multiple_agents_batch(\n",
    "    DDQN_params, \n",
    "    model_params, \n",
    "    hyperparams[\"n_train\"], \n",
    "    outdir, \n",
    "    hyperparams[\"n_runs\"], \n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the strategies\n",
    "\n",
    "Now that the training is complete, we can now continue with evaluating the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOTTING TRAINING...\n",
      "PLOTTING STRATEGIES...\n",
      "EVALUATING AGENTS...\n",
      "EVALUATING BENCHMARKS...\n",
      "\tbest agent\n",
      "\tmean agent\n",
      "\tconstant strategy\n",
      "\trandom_strategy\n",
      "VISUALIZING THE STRATEGIES...\n"
     ]
    }
   ],
   "source": [
    "evaluate_DDQN_batch(\n",
    "    outdir, \n",
    "    n_test=hyperparams[\"n_test\"],                  \n",
    "    Q=10,       # how many depths that should be displayed in the heatmaps\n",
    "    randomize_start=model_params[\"randomize_reset\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now have a look at the images that were saved when running *evaluate\\_DDQN\\_batch*.\n",
    "\n",
    "Let's first have a look at the reward, the estimated state-value at (0,0) and the loss during training.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/training_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "In this image it looks like that the algorithm hasn't converged. Indeed, it has to be trained for much longer. It probably also needs hyperparameter tuning.\n",
    "\n",
    "We can also have a look the learnt strategies. The figure below shows the learnt bid depths.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/bid_heat_randomized_10.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "We can also compare the average rewards of the Q-learning strategies versus benchmarking strategies. These are displayed in the boxplot below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/box_plot_benchmarking.png\"/>\n",
    "</div>\n",
    "\n",
    "We can also view these results in table form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy           mean reward    std reward    reward per action    reward per second\n",
      "---------------  -------------  ------------  -------------------  -------------------\n",
      "constant (d=1)         -0.0022     0.165654             -2.2e-05             -2.2e-05\n",
      "random                 -0.0054     0.0662634            -5.4e-05             -5.4e-05\n",
      "DDQN (best run)         0.0166     0.0549767             0.000166             0.000166\n",
      "DDQN (mean)             0.0055     0.070461              5.5e-05              5.5e-05\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results/mc_model_deep/mc_deep_example/image_folder/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at how the mean strategy and the individual strategies act. The figures below shows the average inventory, cash and value process of the different strategies.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/visualization_mean.png\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/visualization_all.png\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
