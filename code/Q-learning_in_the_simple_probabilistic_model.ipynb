{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simple Probabilistic Model\n",
    "\n",
    "The simple probabilistic model (SPM) is the first model used in our thesis. It is a discretization of the model presentented in Chapter 10.2 in Cartea et. al's book _Algorithmic and High-Frequency Trading_.\n",
    "\n",
    "It can be summarized as follows (for the full definition, see our [report](deadlink)):\n",
    "* The time _t_ can take values between 0 and _T_\n",
    "* The midprice S<sub>t</sub> is a Brownian motion rounded to the closest tick\n",
    "* The market maker can put the bid and ask depths at _d_ different levels, from 0 to _d_ - 1 ticks away from the mid price\n",
    "* The cash process X<sub>t</sub> denotes the market makers cash at time _t_\n",
    "* The inventory process Q<sub>t</sub> denotes the market makers inventory at time _t_\n",
    "* At time _t_ the market maker is forced to liquidate its position\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of AAPL.\n",
    "\n",
    "Based on this definition, an analytically optimal strategy can be defined, with which we want to compare strategies derived with Q-learning. An example of the optimal bid depths for a specific set of model parameters is shown in the figure below. **Note** that these depths are _not_ discreteized.\n",
    "\n",
    "![OptimalBidDepths](images/ContinuousBid30.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Q-learning\n",
    "\n",
    "After that short introduction, it's time for some reinforcement learning in the form of Q-learning.\n",
    "\n",
    "We start by importing the needed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Q-learning file for the simple probabilistic model\n",
    "from simple_model_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to decide on the parameters we want to use for the environment and the hyperparameters we want to use for the Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"d\": 4,         # the number of different depths that can be chosen from\n",
    "                \"T\": 20,        # the length of the episode\n",
    "                \"dp\": 0.01,     # the tick size\n",
    "                \"min_dp\": 0,    # the minimum number of ticks from the mid price that is allowed to put prices at\n",
    "                \"phi\": 1e-4     # the running inventory penalty\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\\_start_ indicates the starting value of the parameter,\n",
    "_\\_end_ indicates the final value of the parameter,\n",
    "_\\_cutoff_ indicate where the final value is reached, i.e. 0.5 mean after 50% of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning_params = {\n",
    "        # epsilon-greedy values (linear decay)\n",
    "        \"epsilon_start\": 1,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_cutoff\": 0.5,\n",
    "\n",
    "        # learning-rate values (exponential decay)\n",
    "        \"alpha_start\": 0.5,\n",
    "        \"alpha_end\": 0.001,\n",
    "        \"alpha_cutoff\": None,\n",
    "\n",
    "        # exploring starts values (linear decay)\n",
    "        \"beta_start\": 1,\n",
    "        \"beta_end\": 0.05,\n",
    "        \"beta_cutoff\": 0.5,\n",
    "        \"exploring_starts\": True\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "        \"n_train\" : 1e5,\n",
    "        \"n_test\" : 1e4,\n",
    "        \"n_runs\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decide where to save our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the folder where the results will be saved\n",
    "folder_mode = True\n",
    "folder_name = \"spm_example\"\n",
    "save_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for the Q-learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 1 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:15.900000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:31.910000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:55.660000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:26.240000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:06.430000\n",
      "0:06:19.300000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 2 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:24.510000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:40.060000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:01:01.650000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:29.790000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:26.520000\n",
      "0:04:53.050000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 3 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:18.700000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:35.560000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:58.220000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:27.570000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:13.260000\n",
      "0:02:13.260000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 4 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:15.330000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:32.920000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:56.140000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:26.560000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:07.830000\n",
      "========================================\n",
      "FULL TRAINING COMPLETED IN 0:08:54.050000\n"
     ]
    }
   ],
   "source": [
    "Q_learning_comparison(\n",
    "    **hyperparams,\n",
    "    args=model_params,\n",
    "    Q_learning_args=Q_learning_params,\n",
    "    folder_mode = folder_mode,\n",
    "    folder_name = folder_name,\n",
    "    save_mode = save_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the strategies\n",
    "\n",
    "We can now have a look at the images that were saved when running _Q\\_learning\\_comparison_.\n",
    "\n",
    "Let's first have a look at the reward and the state-value at (0,0) during training.\n",
    "\n",
    "![TrainingGraph](results/simple_model/spm_example/results_graph.png)\n",
    "\n",
    "In this image it looks like that the Q-learning has converged, however, it has not. It has to be trained for much longer.\n",
    "\n",
    "We can also have a look the learnt strategies. The figure below shows the learnt bid depths.\n",
    "\n",
    "![Q-learningBidDepths](results/simple_model/spm_example/opt_bid_strategy.png)\n",
    "\n",
    "We can also compare the average rewards of the Q-learning strategies versus benchmarking strategies. These are displayed in the boxplot below.\n",
    "\n",
    "![BoxplotsRun](results/simple_model/spm_example/box_plot_benchmarking.png)\n",
    "\n",
    "There are a lot more figures and tables to explore, visit the [spm_example](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/tree/main/code/results/simple_model/spm_example) folder.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
